// Copyright 2022 PingCAP, Ltd.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include <Common/ProfileEvents.h>
#include <Common/TiFlashMetrics.h>
#include <Interpreters/Context.h>
#include <Poco/File.h>
#include <RaftStoreProxyFFI/ColumnFamily.h>
#include <Storages/DeltaMerge/DeltaMergeStore.h>
#include <Storages/DeltaMerge/File/DMFile.h>
#include <Storages/DeltaMerge/File/DMFileBlockOutputStream.h>
#include <Storages/DeltaMerge/SSTFilesToDTFilesOutputStream.h>
#include <Storages/StorageDeltaMerge.h>
#include <Storages/Transaction/PartitionStreams.h>
#include <Storages/Transaction/ProxyFFI.h>
#include <Storages/Transaction/Region.h>
#include <Storages/Transaction/SSTReader.h>
#include <Storages/Transaction/TMTContext.h>
#include <common/logger_useful.h>

namespace ProfileEvents
{
extern const Event DMWriteBytes;
}

namespace DB
{
namespace ErrorCodes
{
extern const int LOGICAL_ERROR;
} // namespace ErrorCodes

namespace DM
{
SSTFilesToDTFilesOutputStream::SSTFilesToDTFilesOutputStream( //
    BoundedSSTFilesToBlockInputStreamPtr child_,
    StorageDeltaMergePtr storage_,
    DecodingStorageSchemaSnapshotConstPtr schema_snap_,
    TiDB::SnapshotApplyMethod method_,
    FileConvertJobType job_type_,
    UInt64 split_after_rows_,
    UInt64 split_after_size_,
    TMTContext & tmt_)
    : child(std::move(child_))
    , //
    storage(std::move(storage_))
    , schema_snap(std::move(schema_snap_))
    , method(method_)
    , job_type(job_type_)
    , split_after_rows(split_after_rows_)
    , split_after_size(split_after_size_)
    , tmt(tmt_)
    , log(&Poco::Logger::get("SSTFilesToDTFilesOutputStream"))
{
}

SSTFilesToDTFilesOutputStream::~SSTFilesToDTFilesOutputStream() = default;

void SSTFilesToDTFilesOutputStream::writePrefix()
{
    child->readPrefix();
    total_committed_rows = 0;
    total_committed_bytes = 0;
    watch.start();
}

void SSTFilesToDTFilesOutputStream::writeSuffix()
{
    child->readSuffix();

    finalizeDTFileStream();

    const auto process_keys = child->getProcessKeys();
    if (job_type == FileConvertJobType::ApplySnapshot)
    {
        GET_METRIC(tiflash_raft_command_duration_seconds, type_apply_snapshot_predecode_sst2dt).Observe(watch.elapsedSeconds());
        // Note that number of keys in different cf will be aggregated into one metrics
        GET_METRIC(tiflash_raft_process_keys, type_apply_snapshot).Increment(process_keys.total());
    }
    else
    {
        // Note that number of keys in different cf will be aggregated into one metrics
        GET_METRIC(tiflash_raft_process_keys, type_ingest_sst).Increment(process_keys.total());
    }

    LOG_FMT_INFO(log, "Begin print all DTFile ranges...");
    const auto files = outputFiles();
    for (const auto & f : files.toUnderType())
    {
        LOG_FMT_INFO(log, "{}", f.toString());
    }
    LOG_FMT_INFO(log, "Finish print all DTFile ranges...");


    LOG_FMT_INFO(
        log,
        "Pre-handle snapshot {} to {} DTFiles, cost {}ms [rows={}] [bytes={}] [write_cf_keys={}] [default_cf_keys={}] [lock_cf_keys={}]",
        child->getRegion()->toString(true),
        ingest_files.size(),
        watch.elapsedMilliseconds(),
        total_committed_rows,
        total_committed_bytes,
        process_keys.write_cf,
        process_keys.default_cf,
        process_keys.lock_cf);
}

bool SSTFilesToDTFilesOutputStream::newDTFileStream()
{
    if (unlikely(dt_stream != nullptr))
    {
        throw Exception("Expect dt_stream to be nullptr", ErrorCodes::LOGICAL_ERROR);
    }

    // Generate a DMFilePtr and its DMFileBlockOutputStream
    DMFileBlockOutputStream::Flags flags;
    switch (method)
    {
    case TiDB::SnapshotApplyMethod::DTFile_Directory:
        flags.setSingleFile(false);
        break;
    case TiDB::SnapshotApplyMethod::DTFile_Single:
        flags.setSingleFile(true);
        break;
    default:
        break;
    }

    // The parent_path and file_id are generated by the storage.
    auto [parent_path, file_id] = storage->getStore()->preAllocateIngestFile();
    if (parent_path.empty())
    {
        // Can no allocate path and id for storing DTFiles (the storage may be dropped / shutdown)
        return false;
    }

    auto dt_file = DMFile::create(file_id, parent_path, flags.isSingleFile(), storage->createChecksumConfig(flags.isSingleFile()));
    dt_stream = std::make_unique<DMFileBlockOutputStream>(tmt.getContext(), dt_file, *(schema_snap->column_defines), flags);
    dt_stream->writePrefix();
    ingest_files.emplace_back(dt_file);
    {
        if (ingest_files_range.empty())
        {
            ingest_files_range.emplace_back(RowKeyRange::newAll(
                schema_snap->is_common_handle,
                schema_snap->rowkey_column_size));
        }
        else
        {
            ingest_files_range.emplace_back(RowKeyRange::startFrom(
                ingest_files_range.back().getEnd(),
                schema_snap->is_common_handle,
                schema_snap->rowkey_column_size));
        }
    }

    committed_rows_this_dt_file = 0;
    committed_bytes_this_dt_file = 0;

    LOG_FMT_INFO(
        log,
        "Create DTFile for snapshot data {} [file_idx={}] [file={}] [single_file_mode={}]",
        child->getRegion()->toString(true),
        ingest_files.size() - 1,
        dt_file->path(),
        flags.isSingleFile());

    return true;
}

bool SSTFilesToDTFilesOutputStream::finalizeDTFileStream()
{
    if (unlikely(dt_stream == nullptr))
    {
        // Maybe error happened in `newDTFileStream`, or no data has been written since last finalize.
        return false;
    }

    dt_stream->writeSuffix();
    auto dt_file = dt_stream->getFile();
    assert(!dt_file->canGC()); // The DTFile should not be able to gc until it is ingested.
    // Add the DTFile to StoragePathPool so that we can restore it later
    const auto bytes_written = dt_file->getBytesOnDisk();
    storage->getStore()->preIngestFile(dt_file->parentPath(), dt_file->fileId(), bytes_written);

    // Report DMWriteBytes for calculating write amplification
    ProfileEvents::increment(ProfileEvents::DMWriteBytes, bytes_written);
    dt_stream.reset();

    LOG_FMT_INFO(
        log,
        "Finished write DTFile for snapshot data {} [file_idx={}] [file={}] [rows={}] [bytes={}] [bytes_on_disk={}]",
        child->getRegion()->toString(true),
        ingest_files.size() - 1,
        dt_file->path(),
        committed_rows_this_dt_file,
        committed_bytes_this_dt_file,
        bytes_written);

    return true;
}

void SSTFilesToDTFilesOutputStream::write()
{
    size_t last_effective_num_rows = 0;
    size_t last_not_clean_rows = 0;
    size_t cur_effective_num_rows = 0;
    size_t cur_not_clean_rows = 0;
    while (true)
    {
        Block block = child->read();
        if (!block)
            break;
        if (unlikely(block.rows() == 0))
            continue;

        LOG_FMT_INFO(
            log,
            "WENXUAN --- SSTFilesToDTFilesOutputStream::write got {} rows ({} bytes) from child",
            block.rows(),
            block.bytes());

        if (dt_stream == nullptr)
        {
            // If can not create DTFile stream (the storage may be dropped / shutdown),
            // break the writing loop.
            if (bool ok = newDTFileStream(); !ok)
            {
                break;
            }
        }

        {
            // Check whether rows are sorted by handle & version in ascending order.
            SortDescription sort;
            sort.emplace_back(MutableSupport::tidb_pk_column_name, 1, 0);
            sort.emplace_back(MutableSupport::version_column_name, 1, 0);
            if (unlikely(block.rows() > 1 && !isAlreadySorted(block, sort)))
            {
                const String error_msg
                    = fmt::format("The block decoded from SSTFile is not sorted by primary key and version {}", child->getRegion()->toString(true));
                LOG_ERROR(log, error_msg);
                FieldVisitorToString visitor;
                const size_t nrows = block.rows();
                for (size_t i = 0; i < nrows; ++i)
                {
                    const auto & pk_col = block.getByName(MutableSupport::tidb_pk_column_name);
                    const auto & ver_col = block.getByName(MutableSupport::version_column_name);
                    LOG_FMT_ERROR(
                        log,
                        "[Row={}/{}] [pk={}] [ver={}]",
                        i,
                        nrows,
                        applyVisitor(visitor, (*pk_col.column)[i]),
                        applyVisitor(visitor, (*ver_col.column)[i]));
                }
                throw Exception(error_msg);
            }
        }

        // TODO: Add checks to ensure that order is maintained intra blocks.
        updateRange(block);

        // Write block to the output stream
        DMFileBlockOutputStream::BlockProperty property;
        std::tie(cur_effective_num_rows, cur_not_clean_rows, property.gc_hint_version) //
            = child->getMvccStatistics();
        property.effective_num_rows = cur_effective_num_rows - last_effective_num_rows;
        property.not_clean_rows = cur_not_clean_rows - last_not_clean_rows;
        last_effective_num_rows = cur_effective_num_rows;
        last_not_clean_rows = cur_not_clean_rows;
        dt_stream->write(block, property);

        auto rows = block.rows();
        auto bytes = block.bytes();
        total_committed_rows += rows;
        total_committed_bytes += bytes;
        committed_rows_this_dt_file += rows;
        committed_bytes_this_dt_file += bytes;
        auto should_split_dt_file = ((split_after_rows > 0 && committed_rows_this_dt_file >= split_after_rows) || //
                                     (split_after_size > 0 && committed_bytes_this_dt_file >= split_after_size));
        if (should_split_dt_file)
        {
            finalizeDTFileStream();
        }
    }
}

SortedExternalDTFileInfos SSTFilesToDTFilesOutputStream::outputFiles() const
{
    if (unlikely(ingest_files.size() != ingest_files_range.size()))
    {
        throw Exception(
            fmt::format(
                "Expect ingest_files.size() == ingest_files_range.size(), but got {} != {}",
                ingest_files.size(),
                ingest_files_range.size()),
            ErrorCodes::LOGICAL_ERROR);
    }

    auto files = std::vector<ExternalDTFileInfo>{};
    files.reserve(ingest_files.size());
    for (size_t i = 0; i < ingest_files.size(); i++)
    {
        files.emplace_back(
            ingest_files[i]->fileId(),
            ingest_files_range[i]);
    }
    return SortedExternalDTFileInfos(std::move(files));
}

void SSTFilesToDTFilesOutputStream::cancel()
{
    // Try a lightweight cleanup the file generated by this stream (marking them able to be GC-ed).
    for (auto & file : ingest_files)
    {
        try
        {
            file->enableGC();
        }
        catch (...)
        {
            tryLogCurrentException(log, fmt::format("ignore exception while canceling SST files to DeltaTree files stream [file={}]", file->path()));
        }
    }
}

void SSTFilesToDTFilesOutputStream::updateRange(Block & block)
{
    // TODO: Now we update the rangeEnd for every block. We can update the rangeEnd only once,
    //  just before a new DTFile is going to be created.

    const auto & pk_col = block.getByName(MutableSupport::tidb_pk_column_name);
    const auto rowkey_column = RowKeyColumnContainer(pk_col.column, schema_snap->is_common_handle);
    auto & current_range = ingest_files_range.back();

    {
        auto const first_handle = rowkey_column.getRowKeyValue(0);
        if (current_range.isStartInfinite())
        {
            current_range.setStart(first_handle);
        }
        else if (unlikely(compare(first_handle, current_range.getStart()) < 0))
        {
            // As our range is right-open, it is very possible that the last range's end == new block's first row.
            // So we only check for the `<` order.
            throw Exception("The new block's first row < previous block's first row");
        }
    }
    {
        auto const last_handle_end = rowkey_column.getRowKeyValue(pk_col.column->size() - 1) //
                                         .toRowKeyValue()
                                         .toPrefixNext(); // because range is right-open.
        if (current_range.isEndInfinite())
        {
            current_range.setEnd(last_handle_end);
        }
        else if (unlikely(compare(last_handle_end.toRowKeyValueRef(), current_range.getEnd()) <= 0))
        {
            throw Exception("The new block's last row <= previous block's last row");
        }
        else
        {
            // the new last_handle > current last_handle, update to the new last_handle.
            current_range.setEnd(last_handle_end);
        }
    }
}

} // namespace DM
} // namespace DB
